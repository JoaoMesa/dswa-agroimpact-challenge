{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c3e10a-a461-46ff-a476-ac4a990a69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from scipy.optimize import differential_evolution\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f7003b-b934-474f-9d28-6d3469d245e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_renomeado.csv')\n",
    "test = pd.read_csv('../data/test_renomeado.csv')\n",
    "\n",
    "train = train.rename(columns={'ENTRADAS (v3)': 'Id'})\n",
    "test = test.rename(columns={'ENTRADAS (v3)': 'Id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26c2677-0b3d-4aca-9b8c-91dffe5cd770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets identificados: 11 colunas\n"
     ]
    }
   ],
   "source": [
    "target_cols = train.columns[-11:].tolist()\n",
    "print(f\"Targets identificados: {len(target_cols)} colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e242c317-b030-4432-956c-66be4b18b3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # 1. Tratamento de outliers e transformações\n",
    "    skewed_cols = ['Emissões pesticidas', 'Urea', 'Macronutrientes']\n",
    "    for col in skewed_cols:\n",
    "        if col in df_new.columns:\n",
    "            df_new[col] = np.log1p(np.abs(df_new[col]))\n",
    "    \n",
    "    # 2. Flags binárias para features com excesso de zeros\n",
    "    zero_flag_cols = ['Energia da biomassa', 'Energia elétrica (kwh)', \n",
    "                      'Esterco animal (kg)', 'Esterco verde (kg)', \n",
    "                      'Micronutrientes', 'Transformation total, to and from']\n",
    "    \n",
    "    for col in zero_flag_cols:\n",
    "        if col in df_new.columns:\n",
    "            df_new[f'flag_{col}'] = (df_new[col] > 0).astype(int)\n",
    "    \n",
    "    # 3. Combinação de features correlacionadas\n",
    "    if 'Calcário e gesso' in df_new.columns and 'Ocuppation, total' in df_new.columns:\n",
    "        df_new['intensidade_calcario'] = df_new['Calcário e gesso'] / (df_new['Ocuppation, total'] + 1e-6)\n",
    "    \n",
    "    if 'Transformation total, to and from' in df_new.columns and 'Energia da biomassa' in df_new.columns:\n",
    "        df_new['eficiencia_energetica'] = df_new['Transformation total, to and from'] / (df_new['Energia da biomassa'] + 1e-6)\n",
    "    \n",
    "    # 4. PCA para features de pesticidas\n",
    "    pesticidas_cols = ['Fungicida, herbicida e pesticida', 'Emissões pesticidas']\n",
    "    if all(col in df_new.columns for col in pesticidas_cols):\n",
    "        pca = PCA(n_components=1)\n",
    "        pesticidas_pca = pca.fit_transform(df_new[pesticidas_cols])\n",
    "        df_new['pesticidas_pca'] = pesticidas_pca\n",
    "    \n",
    "    # 5. Criação de índices compostos\n",
    "    nutrientes_cols = ['Macronutrientes', 'Micronutrientes']\n",
    "    if all(col in df_new.columns for col in nutrientes_cols):\n",
    "        df_new['nutrientes_total'] = df_new['Macronutrientes'] + df_new['Micronutrientes']\n",
    "    \n",
    "    if 'Urea' in df_new.columns and 'Ammonia e afins' in df_new.columns:\n",
    "        df_new['impacto_fertilizantes'] = df_new['Urea'] + 0.7 * df_new['Ammonia e afins']\n",
    "    \n",
    "    # 6. Interações com a cultura (Seed)\n",
    "    if 'Seed' in df_new.columns:\n",
    "        df_new['biomassa_por_seed'] = df_new.groupby('Seed')['Energia da biomassa'].transform('mean')\n",
    "        df_new['calcario_por_seed'] = df_new.groupby('Seed')['Calcário e gesso'].transform('mean')\n",
    "    \n",
    "    # 7. Transformações polinomiais para top features\n",
    "    top_features = [\n",
    "        'Transformation total, to and from',\n",
    "        'Energia da biomassa',\n",
    "        'Micronutrientes',\n",
    "        'Esterco animal (kg)',\n",
    "        'Fungicida, herbicida e pesticida'\n",
    "    ]\n",
    "    \n",
    "    for col in top_features:\n",
    "        if col in df_new.columns:\n",
    "            df_new[f'{col}_sq'] = df_new[col] ** 2\n",
    "            df_new[f'{col}_sqrt'] = np.sqrt(np.abs(df_new[col]))\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e4fa005-506e-4594-a79c-f7e1f2e448c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_featured = create_features(train)\n",
    "test_featured = create_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471c22be-1792-4f21-95d8-5f474227b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical(train_df, test_df):\n",
    "    train_encoded = train_df.copy()\n",
    "    test_encoded = test_df.copy()\n",
    "    \n",
    "    if 'Seed' in train_encoded.columns:\n",
    "        # Target Encoding\n",
    "        target_encoder = {}\n",
    "        for target in target_cols:\n",
    "            for seed in train_encoded['Seed'].unique():\n",
    "                mask = (train_encoded['Seed'] == seed)\n",
    "                target_encoder[(seed, target)] = train_encoded.loc[mask, target].mean()\n",
    "        \n",
    "        # Aplicar encoding\n",
    "        for col in target_cols:\n",
    "            train_encoded[f'Seed_encoded_{col}'] = train_encoded['Seed'].apply(\n",
    "                lambda x: target_encoder.get((x, col), 0)\n",
    "            )\n",
    "            test_encoded[f'Seed_encoded_{col}'] = test_encoded['Seed'].apply(\n",
    "                lambda x: target_encoder.get((x, col), np.nan)\n",
    "            )\n",
    "            \n",
    "            # Preencher valores faltantes\n",
    "            global_mean = train_encoded[col].mean()\n",
    "            test_encoded[f'Seed_encoded_{col}'] = test_encoded[f'Seed_encoded_{col}'].fillna(global_mean)\n",
    "        \n",
    "        # Remover coluna original\n",
    "        train_encoded = train_encoded.drop(columns=['Seed'])\n",
    "        test_encoded = test_encoded.drop(columns=['Seed'])\n",
    "    \n",
    "    return train_encoded, test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de077d14-69c4-4b42-aa8b-75915db4e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded, test_encoded = encode_categorical(train_featured, test_featured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f49aa2aa-6898-463d-b723-47daee78056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = train_encoded['Id']\n",
    "test_ids = test_encoded['Id']\n",
    "\n",
    "# Separar features e targets\n",
    "X_train = train_encoded.drop(columns=['Id'] + target_cols)\n",
    "y_train = train_encoded[target_cols]\n",
    "X_test = test_encoded.drop(columns=['Id'])\n",
    "\n",
    "# Garantir mesmas colunas no treino e teste\n",
    "for col in X_train.columns:\n",
    "    if col not in X_test.columns:\n",
    "        X_test[col] = 0\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "# Normalização\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e97ec6-b385-4efb-bea5-b35777850082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando RandomForest...\n",
      "  Fold 1/5\n",
      "  Fold MSE: 0.02479\n",
      "  Fold 2/5\n",
      "  Fold MSE: 0.30642\n",
      "  Fold 3/5\n",
      "  Fold MSE: 0.13476\n",
      "  Fold 4/5\n",
      "  Fold MSE: 0.44579\n",
      "  Fold 5/5\n",
      "  Fold MSE: 0.21991\n",
      "RandomForest OOF MSE: 0.22603\n",
      "\n",
      "Treinando XGBoost...\n",
      "  Fold 1/5\n",
      "  Fold MSE: 0.01486\n",
      "  Fold 2/5\n",
      "  Fold MSE: 0.34261\n",
      "  Fold 3/5\n",
      "  Fold MSE: 0.11747\n",
      "  Fold 4/5\n",
      "  Fold MSE: 0.43085\n",
      "  Fold 5/5\n",
      "  Fold MSE: 0.26902\n",
      "XGBoost OOF MSE: 0.23464\n",
      "\n",
      "Treinando SVR...\n",
      "  Fold 1/5\n",
      "  Fold MSE: 0.12425\n",
      "  Fold 2/5\n",
      "  Fold MSE: 0.38055\n",
      "  Fold 3/5\n",
      "  Fold MSE: 0.34479\n",
      "  Fold 4/5\n",
      "  Fold MSE: 0.68044\n",
      "  Fold 5/5\n",
      "  Fold MSE: 0.36247\n",
      "SVR OOF MSE: 0.37810\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"RandomForest\": MultiOutputRegressor(\n",
    "        RandomForestRegressor(\n",
    "            n_estimators=300,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            max_features=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    ),\n",
    "    \"XGBoost\": MultiOutputRegressor(\n",
    "        XGBRegressor(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    ),\n",
    "    \"SVR\": MultiOutputRegressor(\n",
    "        SVR(C=3.0, epsilon=0.1, kernel='rbf')\n",
    "    )\n",
    "}\n",
    "\n",
    "# Treinar modelos e fazer previsões OOF (Out-of-Fold)\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "oof_predictions = {name: np.zeros_like(y_train.values) for name in models}\n",
    "test_predictions = {name: np.zeros((X_test.shape[0], len(target_cols))) for name in models}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTreinando {name}...\")\n",
    "    fold_test_preds = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_scaled)):\n",
    "        print(f\"  Fold {fold+1}/{n_splits}\")\n",
    "        X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
    "        y_tr, y_val = y_train.values[train_idx], y_train.values[val_idx]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        # Prever no validation set\n",
    "        val_preds = model.predict(X_val)\n",
    "        oof_predictions[name][val_idx] = val_preds\n",
    "        \n",
    "        # Calcular MSE do fold\n",
    "        fold_mse = mean_squared_error(y_val, val_preds)\n",
    "        print(f\"  Fold MSE: {fold_mse:.5f}\")\n",
    "        \n",
    "        # Prever no teste\n",
    "        fold_test_preds.append(model.predict(X_test_scaled))\n",
    "    \n",
    "    # Média das previsões do teste entre os folds\n",
    "    test_predictions[name] = np.mean(fold_test_preds, axis=0)\n",
    "    \n",
    "    # Calcular MSE completo\n",
    "    full_mse = mean_squared_error(y_train, oof_predictions[name])\n",
    "    print(f\"{name} OOF MSE: {full_mse:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73fd65fb-0708-4d8a-8c32-9c05863390c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otimizando target 1/11\n",
      "  Pesos otimizados: [0.15727786 0.84272214 0.        ]\n",
      "Otimizando target 2/11\n",
      "  Pesos otimizados: [0.47406558 0.52593442 0.        ]\n",
      "Otimizando target 3/11\n",
      "  Pesos otimizados: [0.2931622  0.24416991 0.46266789]\n",
      "Otimizando target 4/11\n",
      "  Pesos otimizados: [0.08708583 0.74240685 0.17050732]\n",
      "Otimizando target 5/11\n",
      "  Pesos otimizados: [0.48179939 0.51820061 0.        ]\n",
      "Otimizando target 6/11\n",
      "  Pesos otimizados: [9.99976897e-01 2.23403965e-06 2.08687994e-05]\n",
      "Otimizando target 7/11\n",
      "  Pesos otimizados: [0.23590977 0.76409023 0.        ]\n",
      "Otimizando target 8/11\n",
      "  Pesos otimizados: [0.97311323 0.02688677 0.        ]\n",
      "Otimizando target 9/11\n",
      "  Pesos otimizados: [0.06359396 0.93640604 0.        ]\n",
      "Otimizando target 10/11\n",
      "  Pesos otimizados: [0.89790568 0.10209432 0.        ]\n",
      "Otimizando target 11/11\n",
      "  Pesos otimizados: [0.96621702 0.03378298 0.        ]\n"
     ]
    }
   ],
   "source": [
    "all_oof_preds = [oof_predictions[name] for name in models]\n",
    "all_test_preds = [test_predictions[name] for name in models]\n",
    "\n",
    "# Função para calcular MSE com pesos\n",
    "def ensemble_mse(weights, preds_list, y_true):\n",
    "    weighted_preds = np.zeros_like(y_true)\n",
    "    for i, w in enumerate(weights):\n",
    "        weighted_preds += w * preds_list[i]\n",
    "    return mean_squared_error(y_true, weighted_preds)\n",
    "\n",
    "# Otimizar pesos para cada target separadamente\n",
    "optimal_weights = np.zeros((len(target_cols), len(models)))\n",
    "\n",
    "for target_idx in range(len(target_cols)):\n",
    "    print(f\"Otimizando target {target_idx+1}/{len(target_cols)}\")\n",
    "    target_oof_preds = [preds[:, target_idx] for preds in all_oof_preds]\n",
    "    \n",
    "    # Definir função objetivo para este target\n",
    "    def target_objective(weights):\n",
    "        return ensemble_mse(weights, target_oof_preds, y_train.values[:, target_idx])\n",
    "    \n",
    "    # Otimizar pesos\n",
    "    bounds = [(0, 1)] * len(models)\n",
    "    result = differential_evolution(\n",
    "        target_objective,\n",
    "        bounds,\n",
    "        strategy='best1bin',\n",
    "        maxiter=100,\n",
    "        popsize=15,\n",
    "        tol=1e-4,\n",
    "        mutation=(0.5, 1),\n",
    "        recombination=0.7,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # Normalizar pesos para soma=1\n",
    "    weights = result.x\n",
    "    weights /= weights.sum()\n",
    "    optimal_weights[target_idx] = weights\n",
    "    print(f\"  Pesos otimizados: {weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f88873b-07ee-4a63-a453-b908cfeeed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Criando previsões finais...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCriando previsões finais...\")\n",
    "final_predictions = np.zeros((X_test_scaled.shape[0], len(target_cols)))\n",
    "\n",
    "for target_idx in range(len(target_cols)):\n",
    "    for model_idx, model_name in enumerate(models):\n",
    "        weight = optimal_weights[target_idx, model_idx]\n",
    "        final_predictions[:, target_idx] += weight * all_test_preds[model_idx][:, target_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dcda5dd-5206-4a31-81ba-fd04e7052997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando arquivo de submissão...\n",
      "Submissão salva em: ../submissions/ensemble_predictions.csv\n",
      "Salvando modelos para uso futuro...\n",
      "Processo completo!\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparando arquivo de submissão...\")\n",
    "submission = pd.DataFrame(final_predictions, columns=[f\"target{i+1}\" for i in range(11)])\n",
    "submission.insert(0, 'Id', test_ids.values)\n",
    "\n",
    "# Garantir que não há valores negativos\n",
    "submission.iloc[:, 1:] = submission.iloc[:, 1:].clip(lower=0)\n",
    "\n",
    "# Salvar arquivo\n",
    "submission_file = '../submissions/ensemble_predictions.csv'\n",
    "submission.to_csv(submission_file, index=False)\n",
    "print(f\"Submissão salva em: {submission_file}\")\n",
    "\n",
    "# 10. ===== Salvar modelos =====\n",
    "print(\"Salvando modelos para uso futuro...\")\n",
    "for name, model in models.items():\n",
    "    joblib.dump(model, f'../models/{name}_model.pkl')\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "np.save('../models/optimal_weights.npy', optimal_weights)\n",
    "\n",
    "print(\"Processo completo!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
